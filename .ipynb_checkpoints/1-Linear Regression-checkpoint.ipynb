{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have previously looked at Facebook data, we will review it again here. To answer our question: \"are posts that are shared more often receive more likes?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_dataset ():\n",
    "    num_rows = sum( 1 for line in open( './data/dataset_Facebook.csv' )) - 1\n",
    "    X = np.zeros((num_rows, 1 ))\n",
    "    y = np.zeros((num_rows, 1 ))\n",
    "    with open( './data/dataset_Facebook.csv' ) as f:\n",
    "        reader = csv.DictReader(f, delimiter= ';' )\n",
    "        next(reader, None )\n",
    "        for i, row in enumerate(reader):\n",
    "            X[i] = int(row[ 'share' ]) if len(row[ 'share' ]) > 0 else 0\n",
    "            y[i] = int(row[ 'like' ]) if len(row[ 'like' ]) > 0 else 0\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the means of our inputs and outputs using a quick numpy function. We can compute the errors in a vectorized fashion as well. In numpy, when we perform addition, subtraction, multiplication, or division of an array by a scalar, numpy will apply that operation to all values in the array. \n",
    "\n",
    "For example, when we subtract the scalar mean_x from the vector X, we're actually taking each element of X and subtracting mean_x from it. Same goes for the vector y. \n",
    "\n",
    "When we compute the errors, we have to tell numpy to do an element-wise multiplication of the two vectors, not a vector-vector multiplication. (Mathematically, this is called the Hadamard product ). This takes the first element of errors_x and multiplies it by the first element of errors_y and so on to produce a new vector. Then we take the sum of all of the elements in that vector. This produces the numerator of the expression to compute the slope.\n",
    "To compute the denominator, we can simply take the square of t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression (object):\n",
    "    #Implements linear regression fit and predict functions\n",
    "    def __init__ (self):\n",
    "        self.w = 0\n",
    "        self.b = 0\n",
    "        self.rho = 0\n",
    "    \n",
    "    # calculate the bias b and the slope w of the best fit\n",
    "    def fit ( self , X, y):\n",
    "        mean_x = X.mean()\n",
    "        mean_y = y.mean()\n",
    "        errors_x = X - mean_x\n",
    "        errors_y = y - mean_y\n",
    "        \n",
    "        # Hadamard product not a vector-vector multiplication\n",
    "        errors_product_xy = np.sum(np.multiply(errors_x, errors_y))\n",
    "        squared_errors_x = np.sum(errors_x ** 2 )\n",
    "        \n",
    "        self .w = errors_product_xy / squared_errors_x\n",
    "        self .b = mean_y - self .w * mean_x\n",
    "        \n",
    "        # calculating covariance: measure how 2 variables vary with respect to each other\n",
    "        N = len(X)\n",
    "        cov = errors_product_xy / N\n",
    "        \n",
    "        # calculating correlation coefficient rho to measure how strong is a relationship between 2 variables\n",
    "        std_x = X.std()\n",
    "        std_y = y.std()\n",
    "        self .rho = cov / (std_x * std_y)\n",
    "    \n",
    "    def predict ( self , X):\n",
    "        # return based on the equation of the best fit line\n",
    "        return self .w * X + self .b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation coefficient is a value from -1 to 1 that represents two thing: the linearity of our data and the correlation between X and Y. A value close to -1 or 1 means that our data is very linear. A value close to 0 means that our data is not linear at all. If the value is positive, then high values of X tend to produce high values of Y and vice-versa. If the value is negative, high values of X tend to produce low values of Y and vice-versa. This is a very powerful metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def visualize_solution (X, y, lin_reg):\n",
    "        plt.xlabel( 'Number of shares' )\n",
    "        plt.ylabel( 'Number of likes' )\n",
    "        plt.scatter(X, y) # explore the data\n",
    "        x = np.arange( 0 , 800 )\n",
    "        \n",
    "        # predicting the targets (linear fit) replace the y\n",
    "        y = lin_reg.predict(x)\n",
    "        \n",
    "        # plot -- in red with laber r=\n",
    "        plt.plot(x, y, 'r--' , label= 'r = %.2f' % lin_reg.rho)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_dataset()\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "visualize_solution(X, y, lin_reg)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that our data has a correlation coefficient of +0.9, which is close to 1 and positive. This means our data is really linear and the more shares a post gets, the more likes it tends to get too ! We've answered our question!\n",
    "\n",
    "To summarize, we learned how to answer a data science question using linear regression, which gives us the line-of-best-fit for our data. We use a cost function to mathematically represent what the \"best\" line is, and we can use optimization to directly solve for the slope and intercept of the line. However, this equation might not be enough. Additionally,\n",
    "we can compute the correlation coefficient to tell us the linearity of our data and the correlation between the inputs and outputs.\n",
    "Linear regression is a fundamental machine learning algorithm and essential to data science!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise:\n",
    "There are many more columns to this data set, and I encourage you to explore all of them using scatter plots, histograms, etc. \n",
    "Try to find hidden correlations! (But remember that correlation does not imply causation!)\n",
    "\n",
    "Use pandas dataframe to read the facebook csv file, explore the columns available, \n",
    "calculate the correlation between the total reach and the total impressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
